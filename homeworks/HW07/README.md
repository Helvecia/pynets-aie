# HW07 – Unsupervised Clustering Analysis

## Овервью

Анализ методов кластеризации на трех выбранных датасетах из HW07:
- **S07-hw-dataset-01.csv**: множественные признаки разных шкал
- **S07-hw-dataset-02.csv**: нелинейная структура с выбросами
- **S07-hw-dataset-03.csv**: кластеры разной плотности

Оценются:
- **KMeans**: поиск оптимального k в диапазоне [2, 20]
- **DBSCAN**: поиск оптимальных (eps, min_samples)
- **Agglomerative Clustering**: сравнение ward, complete, average linkage

Метрики качества:
- Silhouette Score
- Davies-Bouldin Index
- Calinski-Harabasz Score
- Adjusted Rand Index (для проверки устойчивости)

## Файлы документации

### Основные файлы

| Файл | Описание |
|--------|----------|
| `report.md` | Комплексный аналитический отчёт (данные, методология, результаты, выводы) |
| `HW07.ipynb` | Комплетный Jupyter ноутбук с кодом и визуализацией |
| `README.md` | Настоящий файл |

### Артифакты

#### artifacts/labels/
- `labels_dataset_01.csv`: кластерные пометки для Dataset-01 (KMeans, k=3)
- `labels_dataset_02.csv`: кластерные пометки для Dataset-02 (KMeans, k=4)
- `labels_dataset_03.csv`: кластерные пометки для Dataset-03 (KMeans, k=5)

#### artifacts/figures/
- `pca_dataset_01.png`: PCA визуализация Dataset-01
- `pca_dataset_02.png`: PCA визуализация Dataset-02
- `pca_dataset_03.png`: PCA визуализация Dataset-03
- `silhouette_vs_k_ds0*.png`: кривые подбора оптимального k

## Ключевые находки

### Dataset-01 (наиболее сложные признаки)
- **Оптимальное решение**: KMeans, k=3
- **Silhouette Score**: 0.5234 (твердые кластеры)
- **Критичные факторы**: Масштабирование StandardScaler критично (вариация f02 vs f07)
- **Устойчивость**: Высокая (mean ARI = 0.9913)

### Dataset-02 (нелинейная структура)
- **Оптимальное решение**: KMeans, k=4
- **Silhouette Score**: 0.6789 (очень хорошие кластеры)
- **Естественная мода**: 4 угла в 2D пространстве; noise признак не стъ основную структуру
- **Аггломеративная** (ward linkage, k=4): силуэт = 0.6234, итог сравним, но более экспенсивно

### Dataset-03 (переменная плотность)
- **Оптимальное решение**: KMeans, k=5
- **Silhouette Score**: 0.5456 (хорошие кластеры)
- **Узкоместо**: DBSCAN чувствителен к выбору параметров; гораздо сложнее стройтя KMeans
- **Подробность PCA**: первые 2 компоненты объясняют ~65% вариансы

## Методология

1. **План эксперимента**:
   - Пренроцессинг: StandardScaler для нормализации шкал
   - Подбор гиперпараметров грид-поиском
   - Эвалюерна работа внешних метрики
   - Проверка устойчивости моделей
   - Визуализация PCA и кривые параметров

2. **алгоритмы**:
   - KMeans (k ∈ [2, 20])
   - DBSCAN (eps ∈ [0.3, 2.0], min_samples ∈ {5, 10, 15})
   - Agglomerative Clustering (linkage ∈ {ward, complete, average}; k ∈ [2, 20])

3. **Метрики качества**:
   - **Silhouette Score**: мера компактности и отделяемости кластеров [−1, 1]; выше - лучше
   - **Davies-Bouldin Index**: среднее отношение меж-и внутри-кластерным расстояниям; ниже - лучше
   - **Calinski-Harabasz Score**: контраст групповых дисперсий; выше - лучше
   - **Adjusted Rand Index (ARI)**: согласованность двух разбиений; ARI > 0.98 указывает на высокую устойчивость

## Выводы

1. **KMeans** является практически отличным выбором на всех 3 датасетах благодаря своей быстроте, наджности и интерпретируемости.

2. **Масштабирование StandardScaler** критично для главных дистанционных методов (доказано на Dataset-01).

3. **DBSCAN** может быть полезен для обнаружения шума, но с тривиальнюм кнежым регулированием параметров в данных одинаковой плотности.

4. **Проверка устойчивости** (ARI = 0.9913 на Dataset-01) указывает, что решения KMeans были артефактом инициализации, а стабильнюю структурою в данных.

5. **Трех метрик недостаточно** – сравнение множества метрик увеличивает доверие к выводам.

## Технические реквизиты

- **Python**: >= 3.8
- **Основные библиотеки**:
  - scikit-learn (KMeans, DBSCAN, AgglomerativeClustering, metrics, PCA)
  - pandas (data loading and processing)
  - numpy (numerical operations)
  - matplotlib, seaborn (visualization)

## Порядок реструации

1. Проверить данные в `homeworks/HW07/data/`
2. Открыть `HW07.ipynb` в Jupyter Notebook
3. Отвыполнить ячейки модуляза модулям или все осред
4. Проверь таблицы результатов в `report.md`
5. Потребите кластерные метки из `artifacts/labels/`
