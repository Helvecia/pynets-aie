{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HW06 ‚Äì Decision Trees & Ensembles\n",
        "\n",
        "–°–µ–º–∏–Ω–∞—Ä S06: –î–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π –∏ –∞–Ω—Å–∞–º–±–ª–∏ (Bagging / Random Forest / Boosting)\n",
        "\n",
        "**Dataset:** S06-hw-dataset-04.csv (–±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, —Å–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å)\n",
        "\n",
        "**–¶–µ–ª—å:** –ß–µ—Å—Ç–Ω—ã–π ML-—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç–∏, –ø–æ–¥–±–æ—Ä–æ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, roc_auc_score, roc_curve, confusion_matrix,\n",
        "    auc, precision_recall_curve\n",
        ")\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "os.makedirs('artifacts/figures', exist_ok=True)\n",
        "print('‚úì Setup complete')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading & EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('S06-hw-dataset-04.csv')\n",
        "\n",
        "print('Dataset Shape:', df.shape)\n",
        "print('\\nFirst rows:')\n",
        "print(df.head())\n",
        "\n",
        "print('\\nData Info:')\n",
        "print(df.info())\n",
        "\n",
        "print('\\nBasic Stats:')\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\n' + '='*70)\n",
        "print('TARGET DISTRIBUTION')\n",
        "print('='*70)\n",
        "target_counts = df['target'].value_counts()\n",
        "target_props = df['target'].value_counts(normalize=True)\n",
        "\n",
        "print(f'\\nClass 0: {target_counts[0]} ({target_props[0]:.2%})')\n",
        "print(f'Class 1: {target_counts[1]} ({target_props[1]:.2%})')\n",
        "print(f'\\nImbalance ratio: {target_counts[0] / target_counts[1]:.1f}:1')\n",
        "print('\\nNote: Strong class imbalance (fraud-like). Accuracy alone is insufficient!')\n",
        "print('We will use F1 and ROC-AUC as primary metrics.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df.drop(columns=['id', 'target'])\n",
        "y = df['target']\n",
        "\n",
        "print(f'Features: {X.shape[1]} columns')\n",
        "print(f'Feature names (first 10): {list(X.columns[:10])}')\n",
        "print(f'\\nTarget: {y.shape[0]} samples, {y.nunique()} classes')\n",
        "print(f'\\nMissing values: {X.isnull().sum().sum()} in features, {y.isnull().sum()} in target')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "print('='*70)\n",
        "print('TRAIN/TEST SPLIT')\n",
        "print('='*70)\n",
        "print(f'\\nTrain size: {X_train.shape[0]} ({X_train.shape[0]/len(X):.1%})')\n",
        "print(f'Test size: {X_test.shape[0]} ({X_test.shape[0]/len(X):.1%})')\n",
        "print(f'\\nTrain target distribution:')\n",
        "print(y_train.value_counts(normalize=True).round(4))\n",
        "print(f'\\nTest target distribution:')\n",
        "print(y_test.value_counts(normalize=True).round(4))\n",
        "print(f'\\n‚úì Stratification preserved class balance in both splits')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Baseline Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = {}\n",
        "\n",
        "dummy_clf = DummyClassifier(strategy='most_frequent', random_state=RANDOM_STATE)\n",
        "dummy_clf.fit(X_train, y_train)\n",
        "y_pred_dummy = dummy_clf.predict(X_test)\n",
        "y_pred_proba_dummy = dummy_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "results['DummyClassifier'] = {\n",
        "    'accuracy': accuracy_score(y_test, y_pred_dummy),\n",
        "    'f1': f1_score(y_test, y_pred_dummy, zero_division=0),\n",
        "    'roc_auc': roc_auc_score(y_test, y_pred_proba_dummy),\n",
        "    'model': dummy_clf,\n",
        "    'y_pred': y_pred_dummy,\n",
        "    'y_pred_proba': y_pred_proba_dummy,\n",
        "}\n",
        "\n",
        "print('Baseline 1: DummyClassifier (most_frequent)')\n",
        "print(f\"  Accuracy: {results['DummyClassifier']['accuracy']:.4f}\")\n",
        "print(f\"  F1: {results['DummyClassifier']['f1']:.4f}\")\n",
        "print(f\"  ROC-AUC: {results['DummyClassifier']['roc_auc']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_lr = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('lr', LogisticRegression(random_state=RANDOM_STATE, max_iter=1000, class_weight='balanced'))\n",
        "])\n",
        "pipeline_lr.fit(X_train, y_train)\n",
        "y_pred_lr = pipeline_lr.predict(X_test)\n",
        "y_pred_proba_lr = pipeline_lr.predict_proba(X_test)[:, 1]\n",
        "\n",
        "results['LogisticRegression'] = {\n",
        "    'accuracy': accuracy_score(y_test, y_pred_lr),\n",
        "    'f1': f1_score(y_test, y_pred_lr),\n",
        "    'roc_auc': roc_auc_score(y_test, y_pred_proba_lr),\n",
        "    'model': pipeline_lr,\n",
        "    'y_pred': y_pred_lr,\n",
        "    'y_pred_proba': y_pred_proba_lr,\n",
        "    'params': {'class_weight': 'balanced'}\n",
        "}\n",
        "\n",
        "print('\\nBaseline 2: LogisticRegression (with StandardScaler & balanced weights)')\n",
        "print(f\"  Accuracy: {results['LogisticRegression']['accuracy']:.4f}\")\n",
        "print(f\"  F1: {results['LogisticRegression']['f1']:.4f}\")\n",
        "print(f\"  ROC-AUC: {results['LogisticRegression']['roc_auc']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dt_params = {\n",
        "    'max_depth': [3, 5, 7, 10, 15, None],\n",
        "    'min_samples_leaf': [5, 10, 20],\n",
        "    'min_samples_split': [10, 20]\n",
        "}\n",
        "\n",
        "dt_base = DecisionTreeClassifier(random_state=RANDOM_STATE, class_weight='balanced')\n",
        "dt_grid = GridSearchCV(\n",
        "    dt_base, dt_params, cv=5, scoring='roc_auc', n_jobs=-1\n",
        ")\n",
        "dt_grid.fit(X_train, y_train)\n",
        "\n",
        "print('DecisionTree: Hyperparameter Search Results')\n",
        "print(f'Best params: {dt_grid.best_params_}')\n",
        "print(f'Best CV ROC-AUC: {dt_grid.best_score_:.4f}')\n",
        "\n",
        "dt_best = dt_grid.best_estimator_\n",
        "y_pred_dt = dt_best.predict(X_test)\n",
        "y_pred_proba_dt = dt_best.predict_proba(X_test)[:, 1]\n",
        "\n",
        "results['DecisionTree'] = {\n",
        "    'accuracy': accuracy_score(y_test, y_pred_dt),\n",
        "    'f1': f1_score(y_test, y_pred_dt),\n",
        "    'roc_auc': roc_auc_score(y_test, y_pred_proba_dt),\n",
        "    'model': dt_best,\n",
        "    'y_pred': y_pred_dt,\n",
        "    'y_pred_proba': y_pred_proba_dt,\n",
        "    'params': dt_grid.best_params_,\n",
        "    'cv_score': dt_grid.best_score_\n",
        "}\n",
        "\n",
        "print(f\"\\nTest set metrics:\")\n",
        "print(f\"  Accuracy: {results['DecisionTree']['accuracy']:.4f}\")\n",
        "print(f\"  F1: {results['DecisionTree']['f1']:.4f}\")\n",
        "print(f\"  ROC-AUC: {results['DecisionTree']['roc_auc']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf_params = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [5, 10, 15, None],\n",
        "    'min_samples_leaf': [5, 10],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "rf_base = RandomForestClassifier(random_state=RANDOM_STATE, class_weight='balanced', n_jobs=-1)\n",
        "rf_grid = GridSearchCV(\n",
        "    rf_base, rf_params, cv=5, scoring='roc_auc', n_jobs=-1\n",
        ")\n",
        "rf_grid.fit(X_train, y_train)\n",
        "\n",
        "print('RandomForest: Hyperparameter Search Results')\n",
        "print(f'Best params: {rf_grid.best_params_}')\n",
        "print(f'Best CV ROC-AUC: {rf_grid.best_score_:.4f}')\n",
        "\n",
        "rf_best = rf_grid.best_estimator_\n",
        "y_pred_rf = rf_best.predict(X_test)\n",
        "y_pred_proba_rf = rf_best.predict_proba(X_test)[:, 1]\n",
        "\n",
        "results['RandomForest'] = {\n",
        "    'accuracy': accuracy_score(y_test, y_pred_rf),\n",
        "    'f1': f1_score(y_test, y_pred_rf),\n",
        "    'roc_auc': roc_auc_score(y_test, y_pred_proba_rf),\n",
        "    'model': rf_best,\n",
        "    'y_pred': y_pred_rf,\n",
        "    'y_pred_proba': y_pred_proba_rf,\n",
        "    'params': rf_grid.best_params_,\n",
        "    'cv_score': rf_grid.best_score_\n",
        "}\n",
        "\n",
        "print(f\"\\nTest set metrics:\")\n",
        "print(f\"  Accuracy: {results['RandomForest']['accuracy']:.4f}\")\n",
        "print(f\"  F1: {results['RandomForest']['f1']:.4f}\")\n",
        "print(f\"  ROC-AUC: {results['RandomForest']['roc_auc']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gb_params = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_leaf': [5, 10]\n",
        "}\n",
        "\n",
        "gb_base = GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
        "gb_grid = GridSearchCV(\n",
        "    gb_base, gb_params, cv=5, scoring='roc_auc', n_jobs=-1\n",
        ")\n",
        "gb_grid.fit(X_train, y_train)\n",
        "\n",
        "print('GradientBoosting: Hyperparameter Search Results')\n",
        "print(f'Best params: {gb_grid.best_params_}')\n",
        "print(f'Best CV ROC-AUC: {gb_grid.best_score_:.4f}')\n",
        "\n",
        "gb_best = gb_grid.best_estimator_\n",
        "y_pred_gb = gb_best.predict(X_test)\n",
        "y_pred_proba_gb = gb_best.predict_proba(X_test)[:, 1]\n",
        "\n",
        "results['GradientBoosting'] = {\n",
        "    'accuracy': accuracy_score(y_test, y_pred_gb),\n",
        "    'f1': f1_score(y_test, y_pred_gb),\n",
        "    'roc_auc': roc_auc_score(y_test, y_pred_proba_gb),\n",
        "    'model': gb_best,\n",
        "    'y_pred': y_pred_gb,\n",
        "    'y_pred_proba': y_pred_proba_gb,\n",
        "    'params': gb_grid.best_params_,\n",
        "    'cv_score': gb_grid.best_score_\n",
        "}\n",
        "\n",
        "print(f\"\\nTest set metrics:\")\n",
        "print(f\"  Accuracy: {results['GradientBoosting']['accuracy']:.4f}\")\n",
        "print(f\"  F1: {results['GradientBoosting']['f1']:.4f}\")\n",
        "print(f\"  ROC-AUC: {results['GradientBoosting']['roc_auc']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame({\n",
        "    model_name: {\n",
        "        'Accuracy': data['accuracy'],\n",
        "        'F1': data['f1'],\n",
        "        'ROC-AUC': data['roc_auc']\n",
        "    }\n",
        "    for model_name, data in results.items()\n",
        "}).T\n",
        "\n",
        "print('='*70)\n",
        "print('FINAL TEST METRICS - ALL MODELS')\n",
        "print('='*70)\n",
        "print(results_df.round(4))\n",
        "\n",
        "best_model_name = results_df['ROC-AUC'].idxmax()\n",
        "best_roc_auc = results_df['ROC-AUC'].max()\n",
        "print(f'\\nüèÜ BEST MODEL: {best_model_name}')\n",
        "print(f'   ROC-AUC: {best_roc_auc:.4f}')\n",
        "print(f'   Accuracy: {results_df.loc[best_model_name, \"Accuracy\"]:.4f}')\n",
        "print(f'   F1: {results_df.loc[best_model_name, \"F1\"]:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Diagnostics: ROC Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "for model_name, data in results.items():\n",
        "    fpr, tpr, _ = roc_curve(y_test, data['y_pred_proba'])\n",
        "    roc_auc = data['roc_auc']\n",
        "    plt.plot(fpr, tpr, label=f'{model_name} (AUC={roc_auc:.3f})', linewidth=2)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random (AUC=0.500)', linewidth=1.5)\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curves ‚Äì All Models (Test Set)', fontsize=13, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('artifacts/figures/roc_curves.png', dpi=100, bbox_inches='tight')\n",
        "plt.show()\n",
        "print('‚úì ROC curves saved')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = results[best_model_name]['model']\n",
        "best_y_pred = results[best_model_name]['y_pred']\n",
        "cm = confusion_matrix(y_test, best_y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Class 0', 'Class 1'],\n",
        "            yticklabels=['Class 0', 'Class 1'],\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.title(f'Confusion Matrix ‚Äì {best_model_name}', fontsize=13, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('artifacts/figures/confusion_matrix.png', dpi=100, bbox_inches='tight')\n",
        "plt.show()\n",
        "print('‚úì Confusion matrix saved')\n",
        "\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f'\\nCM Analysis: TN={tn}, FP={fp}, FN={fn}, TP={tp}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Permutation Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "perm_importance = permutation_importance(\n",
        "    best_model, X_test, y_test, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1\n",
        ")\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance_mean': perm_importance.importances_mean,\n",
        "    'importance_std': perm_importance.importances_std\n",
        "}).sort_values('importance_mean', ascending=False).head(15)\n",
        "\n",
        "print('\\nTop-15 Features by Permutation Importance:')\n",
        "print(importance_df.to_string(index=False))\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.barh(range(len(importance_df)), importance_df['importance_mean'], \n",
        "         xerr=importance_df['importance_std'], color='steelblue', capsize=5)\n",
        "plt.yticks(range(len(importance_df)), importance_df['feature'], fontsize=10)\n",
        "plt.xlabel('Permutation Importance (Mean ¬± Std)', fontsize=12)\n",
        "plt.title(f'Top-15 Features ‚Äì {best_model_name}', fontsize=13, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.savefig('artifacts/figures/permutation_importance.png', dpi=100, bbox_inches='tight')\n",
        "plt.show()\n",
        "print('‚úì Permutation importance plot saved')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Save Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_test = {}\n",
        "for model_name, data in results.items():\n",
        "    metrics_test[model_name] = {\n",
        "        'accuracy': float(data['accuracy']),\n",
        "        'f1': float(data['f1']),\n",
        "        'roc_auc': float(data['roc_auc'])\n",
        "    }\n",
        "\n",
        "with open('artifacts/metrics_test.json', 'w') as f:\n",
        "    json.dump(metrics_test, f, indent=2)\n",
        "print('‚úì Test metrics saved')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search_summaries = {}\n",
        "for model_name in ['DecisionTree', 'RandomForest', 'GradientBoosting']:\n",
        "    if 'params' in results[model_name]:\n",
        "        search_summaries[model_name] = {\n",
        "            'best_params': results[model_name]['params'],\n",
        "            'best_cv_score': float(results[model_name].get('cv_score', 0))\n",
        "        }\n",
        "\n",
        "with open('artifacts/search_summaries.json', 'w') as f:\n",
        "    json.dump(search_summaries, f, indent=2)\n",
        "print('‚úì Hyperparameter search summaries saved')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(best_model, 'artifacts/best_model.joblib')\n",
        "print(f'‚úì Best model ({best_model_name}) saved')\n",
        "\n",
        "best_model_meta = {\n",
        "    'model_name': best_model_name,\n",
        "    'test_metrics': metrics_test[best_model_name],\n",
        "    'hyperparameters': results[best_model_name].get('params', {}),\n",
        "    'cv_score': float(results[best_model_name].get('cv_score', 0))\n",
        "}\n",
        "\n",
        "with open('artifacts/best_model_meta.json', 'w') as f:\n",
        "    json.dump(best_model_meta, f, indent=2)\n",
        "print('‚úì Best model metadata saved')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\n' + '='*70)\n",
        "print('EXPERIMENT SUMMARY')\n",
        "print('='*70)\n",
        "print(f'\\nDataset: S06-hw-dataset-04.csv')\n",
        "print(f'  Samples: {len(df)}, Features: {X.shape[1]}')\n",
        "print(f'  Class imbalance: {len(df[df.target==0])/len(df[df.target==1]):.1f}:1')\n",
        "print(f'\\nüèÜ Best Model: {best_model_name} (ROC-AUC={best_roc_auc:.4f})')\n",
        "print(f'\\n‚úÖ HW06 completed successfully!')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
